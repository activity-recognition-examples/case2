{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1338)\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "cmap_group = plt.cm.Paired\n",
    "cmap_y = plt.cm.coolwarm\n",
    "\n",
    "def visualize_groups(classes, groups):\n",
    "    # Visualize dataset groups\n",
    "    fig, ax = plt.subplots(dpi=200) \n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [0.5] * len(groups),\n",
    "        c=groups,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [3.5] * len(groups),\n",
    "        c=classes,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.set(\n",
    "        ylim=[-1, 5],\n",
    "        yticks=[0.5, 3.5],\n",
    "        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n",
    "        xlabel=\"Sample index\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data \n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 2) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[n_splits + 2.2, -0.2],\n",
    "        xlim=[0, len(X)],\n",
    "    )\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(data, target_x, target_y, target_group,  window_size=80, step_size=40):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    group = []\n",
    "\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        segments_data = []\n",
    "        for x in target_x:\n",
    "            x_values = data[x].values[i:i + window_size]\n",
    "            segments_data.append(x_values)\n",
    "        label = data[target_y].values[i]\n",
    "        member = data[target_group].values[i]\n",
    "\n",
    "        segments.append(segments_data)\n",
    "        labels.append(label)\n",
    "        group.append(member)\n",
    "    return segments, labels, group\n",
    "\n",
    "\n",
    "def create_label_segments(data, target, window_size=80, step_size=40):\n",
    "    ys = []\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        segments_data = []\n",
    "        y = data[target].values[i]\n",
    "        y.append(y)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def extract_time_features(segments):\n",
    "    features = []\n",
    "    for segment in segments:\n",
    "        segment_features = []\n",
    "        for axis in segment:\n",
    "            mean_axis = np.mean(axis)\n",
    "            std_axis = np.std(axis)\n",
    "            segment_features.extend([mean_axis, std_axis])\n",
    "        features.append(segment_features)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_frequency_features(segments, sampling_rate=100):\n",
    "    features = []\n",
    "    for segment in segments:\n",
    "        segment_features = []\n",
    "        for axis in segment:\n",
    "            fft_axis = np.abs(fft(axis))\n",
    "\n",
    "            # Peak frequency and maximum amplitude\n",
    "            peak_indices, _ = find_peaks(fft_axis)\n",
    "\n",
    "            if len(peak_indices) > 0:\n",
    "                peak_freq = peak_indices[np.argmax(fft_axis[peak_indices])] / len(axis) * sampling_rate\n",
    "                max_amplitude = np.max(fft_axis[peak_indices])\n",
    "            else:\n",
    "                peak_freq = 0.0  # デフォルトの値を設定\n",
    "                max_amplitude = 0.0  # デフォルトの値を設定\n",
    "\n",
    "            # Signal energy\n",
    "            energy = np.sum(axis ** 2) / len(axis)\n",
    "\n",
    "            segment_features.extend([peak_freq, max_amplitude, energy])\n",
    "\n",
    "        features.append(segment_features)\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(df, target_name):\n",
    "    label_counts = df[target_name].value_counts()\n",
    "\n",
    "    # 棒グラフで表示\n",
    "    label_counts.plot(kind='bar')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Label')\n",
    "    plt.title('Number of data points per label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トイデータの読み込み\n",
    "data = pd.read_csv('02_shortstick_toy_dataset.csv', index_col=0)\n",
    "\n",
    "# 欠損値の除去\n",
    "data.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "\n",
    "# セグメントを用意し、時間領域の特徴量と周波数領域の特徴量を抽出\n",
    "window_size = 80\n",
    "step_size = 40\n",
    "target_x = ['Quaternion.X', 'Quaternion.Y', 'Quaternion.Z']\n",
    "target_y = \"action\" \n",
    "terget_group = \"user_id\"\n",
    "target_names = list(data[target_y].unique())\n",
    "\n",
    "# ラベルエンコーダーのインスタンス化\n",
    "le0 = LabelEncoder()\n",
    "data[\"action\"] = le0.fit_transform(data[\"action\"])\n",
    "\n",
    "le1 = LabelEncoder()\n",
    "data[\"user_id\"] = le1.fit_transform(data[\"user_id\"])\n",
    "\n",
    "segments, labels, groups = create_segments(data, target_x, target_y, terget_group, window_size, step_size)\n",
    "\n",
    "#特徴量抽出\n",
    "time_features = extract_time_features(segments)\n",
    "frequency_features = extract_frequency_features(segments)\n",
    "\n",
    "# 時間領域の特徴量と周波数領域の特徴量を統合\n",
    "combined_features = np.concatenate((time_features, frequency_features), axis=1)\n",
    "\n",
    "# データセットの分割\n",
    "X = combined_features\n",
    "y = np.asarray(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# パラメータのグリッドを定義\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "# グリッドサーチの初期化\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
    "\n",
    "# グリッドサーチの実行\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 最適なパラメータの表示\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# 最適なパラメータで学習したモデルの取得\n",
    "best_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7574791564492398\n",
      "0.8233838540055676\n",
      "0.7697197341808726\n",
      "0.7032238626092197\n",
      "0.7548953409858203\n",
      "Confusion Matrix:\n",
      "[[3362   35   91   19  296   26   18  192]\n",
      " [ 101 1322   24  102  234    3    2   60]\n",
      " [ 246   43 1792  257   48  138   19   18]\n",
      " [  15  110  176 2679    7  134   21   12]\n",
      " [ 241  156   12    0 1185   11    0   31]\n",
      " [ 116   47  167  219   57 1809  166   13]\n",
      " [  32    0   56   33    0  181 1147    4]\n",
      " [ 265   83   10    7   69    8    0  807]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       exe-a       0.77      0.83      0.80      4039\n",
      "       exe-b       0.74      0.72      0.73      1848\n",
      "       exe-c       0.77      0.70      0.73      2561\n",
      "       exe-d       0.81      0.85      0.83      3154\n",
      "       exe-e       0.62      0.72      0.67      1636\n",
      "       exe-f       0.78      0.70      0.74      2594\n",
      "       exe-g       0.84      0.79      0.81      1453\n",
      "       exe-h       0.71      0.65      0.68      1249\n",
      "\n",
      "    accuracy                           0.76     18534\n",
      "   macro avg       0.75      0.74      0.75     18534\n",
      "weighted avg       0.76      0.76      0.76     18534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LeaveOneGroupOutのインスタンスを作成\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# 分割数、ここでは5分割\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "\n",
    "    # モデルの学習\n",
    "    best_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # 予測\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "\n",
    "    # 保存\n",
    "    all_y_true.extend(y_test)\n",
    "    all_y_pred.extend(y_pred)\n",
    "    \n",
    "    # 予測結果の評価（ここでは正解率）\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "all_y_true = le0.inverse_transform(np.array(all_y_true).astype(np.int32))\n",
    "all_y_pred = le0.inverse_transform(np.array(all_y_pred).astype(np.int32))\n",
    "\n",
    "# 全ユーザの結果に基づく混同行列と分類レポートを表示\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_y_true, all_y_pred, labels=target_names))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_y_true, all_y_pred, labels=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
